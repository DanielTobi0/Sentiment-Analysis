{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EpAL0nh1Cm4a0sdAhZ9lAR_nvzF8P-EI",
      "authorship_tag": "ABX9TyPI88ReREHk7uKoaROLv81y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Predictive Modelling for Sentiment Classification\n",
        "\n",
        "Build a machine learning model to predict sentiment (positive or negative) based on review text.\n"
      ],
      "metadata": {
        "id": "Aluvadu7I3PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis with Logistic Regression\n",
        "\n",
        "Overview:\n",
        "------------\n",
        "In this notebook, we perform sentiment analysis on Amazon sentiment dataset using a Logistic Regression model. The goal is to predict whether a given review is positive or negative based on the review text. The dataset contains labeled reviews with corresponding sentiments.\n",
        "\n",
        "Workflow:\n",
        "------------\n",
        "1. **Data Loading:** We load the sentiment dataset, which includes columns for the review class, review title, and review text.\n",
        "\n",
        "2. **Data Preprocessing:** The review title and text are combined, and the class labels are mapped to binary values (positive: 1, negative: 0). The data is then cleaned, removing unnecessary elements such as URLs, HTML tags, and special characters. A stemming process is applied to reduce words to their root form.\n",
        "\n",
        "3. **Train-Test Split:** The dataset is split into training and testing sets to train and evaluate the machine learning model.\n",
        "\n",
        "4. **Text Vectorization:** The full_review text is converted into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization. This step is crucial for training machine learning models on text data.\n",
        "\n",
        "5. **Model Training:** A Logistic Regression model is chosen for its simplicity and effectiveness in binary classification tasks. The model is trained on the training set.\n",
        "\n",
        "6. **Model Evaluation:** The trained model is evaluated on the testing set using accuracy as the performance metric. Additionally, a confusion matrix is generated to analyze the model's performance in terms of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "------------\n"
      ],
      "metadata": {
        "id": "e78zYYLpI953"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6_C3jp-60tEI"
      },
      "outputs": [],
      "source": [
        "# Importing Libaries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because of how large the dataset is, I have reduced the number of rows to save computational time."
      ],
      "metadata": {
        "id": "Ny0_mvveOTOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Data\n",
        "col_names = ['class', 'review_title', 'review_text']\n",
        "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Prembly/Datasets/train.csv', names=col_names)\n",
        "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Prembly/Datasets/test.csv', names=col_names)\n",
        "\n",
        "train_1 = train.iloc[:10000]\n",
        "test_1 = test.iloc[:10000]\n",
        "\n",
        "train_1.shape, test_1.shape"
      ],
      "metadata": {
        "id": "Dyq5nnWtsbVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc09094-848d-4591-9c5b-665ce1baf3ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 3), (10000, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine 'review_title' and 'review_text'\n",
        "train_1['full_review'] = train_1['review_title'] +  ' ' + train_1['review_text']\n",
        "test_1['full_review'] = test_1['review_title'] +  ' ' + test_1['review_text']\n",
        "\n",
        "# Rename class labels: positive 2 to 1, negative 1 to 0\n",
        "train_1['class'] = train_1['class'].map({2: 1, 1: 0})\n",
        "test_1['class'] = test_1['class'].map({2: 1, 1: 0})\n",
        "\n",
        "\n",
        "# Drop columns\n",
        "del train_1['review_title'], train_1['review_text']\n",
        "del test_1['review_title'], test_1['review_text']\n",
        "\n",
        "train_1.dropna(inplace=True)\n",
        "test_1.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "Wncg_WgPstT5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def preprocess_data(data):\n",
        "\n",
        "    #removal of url\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ',data)\n",
        "\n",
        "    #decontraction\n",
        "    text = re.sub(r\"won\\'t\", \" will not\", text)\n",
        "    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n",
        "    text = re.sub(r\"can\\'t\", \" can not\", text)\n",
        "    text = re.sub(r\"don\\'t\", \" do not\", text)\n",
        "    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n",
        "    text = re.sub(r\"ma\\'am\", \" madam\", text)\n",
        "    text = re.sub(r\"let\\'s\", \" let us\", text)\n",
        "    text = re.sub(r\"ain\\'t\", \" am not\", text)\n",
        "    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n",
        "    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n",
        "    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n",
        "    text = re.sub(r\"y\\'all\", \" you all\", text)\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"n\\'t've\", \" not have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'d've\", \" would have\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ll've\", \" will have\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "\n",
        "    #removal of html tags\n",
        "    text = re.sub(r'<.*?>',' ',text)\n",
        "\n",
        "    # Match all digits in the string and replace them by empty string\n",
        "    text = re.sub(r'[0-9]', '', text)\n",
        "    text = re.sub(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\",' ',text)\n",
        "\n",
        "    # filtering out miscellaneous text.\n",
        "    text = re.sub('[^a-zA-Z]',' ',text)\n",
        "    text = re.sub(r\"\\([^()]*\\)\", \"\", text)\n",
        "\n",
        "    # remove mentions\n",
        "    text = re.sub('@\\S+', '', text)\n",
        "\n",
        "    # remove punctuations\n",
        "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', text)\n",
        "\n",
        "\n",
        "    # Lowering all the words in text\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "\n",
        "    text = [stemmer.stem(words) for words in text if words not in STOPWORDS]\n",
        "\n",
        "    # Removal of words with length<2\n",
        "    text = [i for i in text if len(i)>2]\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "train_1[\"Cleaned_review\"] = train_1[\"full_review\"].apply(preprocess_data)\n",
        "test_1[\"Cleaned_review\"] = test_1[\"full_review\"].apply(preprocess_data)"
      ],
      "metadata": {
        "id": "4PzHkoDitc4b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1.iloc[9998]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4F47rXBuTbc",
        "outputId": "3264f58f-c08a-452e-d966-fe66e5746417"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class                                                             0\n",
              "full_review       Don't buy The box looked used and it is obviou...\n",
              "Cleaned_review    buy box look use obvious new tri contact email...\n",
              "Name: 9998, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I noticed that after the cleaning the data, the word 'Don\"t' was removed, words like this would really be useful to categorize negative reviews, because they are more liable to be used for negative reviews."
      ],
      "metadata": {
        "id": "MdcOGgn9xSUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "wUJYuShGycES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_1['Cleaned_review'], train_1['class'], test_size=0.2, random_state=0, stratify=train_1['class'])\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "X-JZbQQYyX5A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cpnverts text data to numeric\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "huKTSYVazmq4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "y_pred = lr.predict(X_test_tfidf)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "vU0x3rzMzxaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f63a17-cc73-4a7e-e36d-c403a37ce34b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8615"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "6qt6Tpa5zxey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd2254d-d7a2-4e1d-d7c6-7861cab2439b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[864 155]\n",
            " [122 859]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test data"
      ],
      "metadata": {
        "id": "WlVfDjzKIax0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cpnverts text data to numeric\n",
        "X_valid = tfidf_vectorizer.transform(test_1['Cleaned_review'])\n",
        "y_valid = np.array(test_1['class'])\n",
        "\n",
        "# Model Evaluation\n",
        "y_valid_pred = lr.predict(X_valid)\n",
        "accuracy_score(y_valid, y_valid_pred)"
      ],
      "metadata": {
        "id": "KvsjEvgtzxjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac128b69-629f-43ae-f746-45c1d04932ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8448844884488449"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_valid, y_valid_pred))"
      ],
      "metadata": {
        "id": "eyeaMC_8zxmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b938e8-78f0-4678-ec61-cdfa4b15fa87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4039  835]\n",
            " [ 716 4409]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference:\n",
        "\n",
        "\n",
        "\n",
        "*   regex code [Link](https://www.kaggle.com/code/mohitnirgulkar/disaster-tweets-classification-using-ml)\n",
        "*   Text Enhancement with ChatGPT\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AMkbhxzmILx-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5o0AyeztPbFw"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}